{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "19d3530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4f6b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted vegetation indices and variability features from drone imagery:\n",
      "  polygon  date    filename   mean_ExG  mean_GLI  mean_VARI   mean_TGI  \\\n",
      "0  Munda1  None  Munda1.tif  53.546490  0.150254   0.023449  32.256836   \n",
      "1  Munda2  None  Munda2.tif  41.190311  0.100614  -0.054592  27.334774   \n",
      "2  Munda3  None  Munda3.tif  46.276634  0.120017  -0.044731  29.833282   \n",
      "3  Munda4  None  Munda4.tif  49.481083  0.133959  -0.028991  31.134373   \n",
      "4  Munda5  None  Munda5.tif  58.587349  0.154823   0.029838  34.988140   \n",
      "\n",
      "   mean_CIVE    mean_Red  mean_Green  mean_Blue    std_ExG  std_Green  \\\n",
      "0  18.681017  126.814560  128.662354  76.963638  24.812695  60.642323   \n",
      "1  18.714027  146.354401  136.314972  85.085243  19.989790  57.192657   \n",
      "2  18.702696  133.024643  125.731285  72.161270  19.136429  53.785713   \n",
      "3  18.693714  128.715103  124.392838  70.589462  21.671139  60.535816   \n",
      "4  18.678141  129.899765  133.309601  78.132088  24.228174  53.942223   \n",
      "\n",
      "   percent_green_cover  \n",
      "0            55.827362  \n",
      "1            40.566476  \n",
      "2            63.499409  \n",
      "3            42.832816  \n",
      "4            49.963518  \n",
      "\n",
      "Unique polygons found in drone data:\n",
      "['Munda1' 'Munda2' 'Munda3' 'Munda4' 'Munda5' 'Munda6' 'Munda7']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Base folder containing all drone image files\n",
    "image_folder = r\"D:\\Yield\\LR\\DroneData\"\n",
    "records = []\n",
    "\n",
    "# List all .tif files within the specified drone image folder\n",
    "files = [f for f in os.listdir(image_folder) if f.endswith('.tif')]\n",
    "files.sort()\n",
    "\n",
    "for file in files:\n",
    "    file_path = os.path.join(image_folder, file)\n",
    "\n",
    "    try:\n",
    "        with rasterio.open(file_path) as src:\n",
    "            red = src.read(1).astype('float32')\n",
    "            green = src.read(2).astype('float32')\n",
    "            blue = src.read(3).astype('float32')\n",
    "\n",
    "        # Calculate Vegetation Indices\n",
    "        exg = 2 * green - red - blue\n",
    "        gli = (2 * green - red - blue) / (2 * green + red + blue + 1e-10)\n",
    "        vari = (green - red) / (green + red - blue + 1e-10)\n",
    "        tgi = green - 0.39 * red - 0.61 * blue\n",
    "        sum_rgb = red + green + blue + 1e-10\n",
    "        r_norm = red / sum_rgb\n",
    "        g_norm = green / sum_rgb\n",
    "        b_norm = blue / sum_rgb\n",
    "        cive = 0.441 * r_norm - 0.881 * g_norm + 0.385 * b_norm + 18.78745\n",
    "\n",
    "        # Clean up potential infinite or NaN values\n",
    "        exg = np.where(np.isfinite(exg), exg, np.nan)\n",
    "        gli = np.where(np.isfinite(gli), gli, np.nan)\n",
    "        vari = np.where(np.isfinite(vari), vari, np.nan)\n",
    "        tgi = np.where(np.isfinite(tgi), tgi, np.nan)\n",
    "        cive = np.where(np.isfinite(cive), cive, np.nan)\n",
    "\n",
    "        # Implement Masking for Green Vegetation (e.g., ExG > threshold)\n",
    "        vegetation_mask = exg > 10 \n",
    "\n",
    "        # Calculate percentage of green cover\n",
    "        total_valid_pixels = np.sum(~np.isnan(exg))\n",
    "        percent_green_cover = (np.sum(vegetation_mask) / total_valid_pixels) * 100 if total_valid_pixels > 0 else np.nan\n",
    "\n",
    "        # Apply the mask to all bands and indices for mean calculation\n",
    "        red_masked = np.where(vegetation_mask, red, np.nan)\n",
    "        green_masked = np.where(vegetation_mask, green, np.nan)\n",
    "        blue_masked = np.where(vegetation_mask, blue, np.nan)\n",
    "        exg_masked = np.where(vegetation_mask, exg, np.nan)\n",
    "        gli_masked = np.where(vegetation_mask, gli, np.nan)\n",
    "        vari_masked = np.where(vegetation_mask, vari, np.nan)\n",
    "        tgi_masked = np.where(vegetation_mask, tgi, np.nan)\n",
    "        cive_masked = np.where(vegetation_mask, cive, np.nan)\n",
    "\n",
    "        # Calculate mean value for each masked index and raw band\n",
    "        mean_exg_masked = np.nanmean(exg_masked)\n",
    "        mean_gli_masked = np.nanmean(gli_masked)\n",
    "        mean_vari_masked = np.nanmean(vari_masked)\n",
    "        mean_tgi_masked = np.nanmean(tgi_masked)\n",
    "        mean_cive_masked = np.nanmean(cive_masked)\n",
    "        mean_red_masked = np.nanmean(red_masked)\n",
    "        mean_green_masked = np.nanmean(green_masked)\n",
    "        mean_blue_masked = np.nanmean(blue_masked)\n",
    "\n",
    "        # Add Variability Features (Standard Deviation)\n",
    "        std_exg_masked = np.nanstd(exg_masked)\n",
    "        std_green_masked = np.nanstd(green_masked)\n",
    "\n",
    "        # Extract polygon name from filename\n",
    "        try:\n",
    "            polygon = file.split('.')[0]\n",
    "            date = None\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting polygon name from '{file}': {e}. Polygon set to None.\")\n",
    "            polygon = None\n",
    "            date = None\n",
    "\n",
    "        # Append extracted features and metadata to records list\n",
    "        records.append({\n",
    "            'polygon': polygon,\n",
    "            'date': date,\n",
    "            'filename': file,\n",
    "            'mean_ExG': mean_exg_masked,\n",
    "            'mean_GLI': mean_gli_masked,\n",
    "            'mean_VARI': mean_vari_masked,\n",
    "            'mean_TGI': mean_tgi_masked,\n",
    "            'mean_CIVE': mean_cive_masked,\n",
    "            'mean_Red': mean_red_masked,\n",
    "            'mean_Green': mean_green_masked,\n",
    "            'mean_Blue': mean_blue_masked,\n",
    "            'std_ExG': std_exg_masked,\n",
    "            'std_Green': std_green_masked,\n",
    "            'percent_green_cover': percent_green_cover\n",
    "        })\n",
    "\n",
    "    except rasterio.errors.RasterioIOError as e:\n",
    "        print(f\"Error opening or reading raster file '{file_path}': {e}. Skipping this file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while processing '{file_path}': {e}. Skipping this file.\")\n",
    "\n",
    "# Create DataFrame from collected records\n",
    "df_indices = pd.DataFrame(records)\n",
    "\n",
    "print(\"✅ Extracted vegetation indices and variability features from drone imagery:\")\n",
    "print(df_indices.head())\n",
    "print(\"\\nUnique polygons found in drone data:\")\n",
    "print(df_indices['polygon'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a4aa69e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Debugging Polygon Names for Merge ---\n",
      "Unique polygons in df_indices (from drone data processing):\n",
      "['Munda1' 'Munda2' 'Munda3' 'Munda4' 'Munda5' 'Munda6' 'Munda7']\n",
      "\n",
      "Unique polygons in df_area (from polygon_areas.csv):\n",
      "['Munda1' 'Munda2' 'Munda3' 'Munda4' 'Munda5' 'Munda6' 'Munda7' 'Munda8']\n",
      "\n",
      "Polygons common to both df_indices and df_area: ['Munda2', 'Munda5', 'Munda4', 'Munda7', 'Munda3', 'Munda6', 'Munda1']\n",
      "All polygons from drone data (df_indices) are present in polygon_areas.csv.\n",
      "--- Debugging Polygon Names for Merge End ---\n",
      "\n",
      "✅ Merged drone vegetation index features with polygon area data:\n",
      "  polygon  date    filename   mean_ExG  mean_GLI  mean_VARI   mean_TGI  \\\n",
      "0  Munda1  None  Munda1.tif  53.546490  0.150254   0.023449  32.256836   \n",
      "1  Munda2  None  Munda2.tif  41.190311  0.100614  -0.054592  27.334774   \n",
      "2  Munda3  None  Munda3.tif  46.276634  0.120017  -0.044731  29.833282   \n",
      "3  Munda4  None  Munda4.tif  49.481083  0.133959  -0.028991  31.134373   \n",
      "4  Munda5  None  Munda5.tif  58.587349  0.154823   0.029838  34.988140   \n",
      "\n",
      "   mean_CIVE    mean_Red  mean_Green  mean_Blue    std_ExG  std_Green  \\\n",
      "0  18.681017  126.814560  128.662354  76.963638  24.812695  60.642323   \n",
      "1  18.714027  146.354401  136.314972  85.085243  19.989790  57.192657   \n",
      "2  18.702696  133.024643  125.731285  72.161270  19.136429  53.785713   \n",
      "3  18.693714  128.715103  124.392838  70.589462  21.671139  60.535816   \n",
      "4  18.678141  129.899765  133.309601  78.132088  24.228174  53.942223   \n",
      "\n",
      "   percent_green_cover  area_ha_x  area_ha_y   area_ha  \n",
      "0            55.827362   0.101974   0.101974  0.101974  \n",
      "1            40.566476   0.339510   0.339510  0.339510  \n",
      "2            63.499409   0.308073   0.308073  0.308073  \n",
      "3            42.832816   0.289367   0.289367  0.289367  \n",
      "4            49.963518   0.185417   0.185417  0.185417  \n"
     ]
    }
   ],
   "source": [
    "# Load polygon area data\n",
    "df_area = pd.read_csv(\"polygon_areas.csv\")\n",
    "\n",
    "# Debugging Polygon Names for Merge\n",
    "print(\"\\n--- Debugging Polygon Names for Merge ---\")\n",
    "print(\"Unique polygons in df_indices (from drone data processing):\")\n",
    "print(df_indices['polygon'].unique())\n",
    "print(\"\\nUnique polygons in df_area (from polygon_areas.csv):\")\n",
    "print(df_area['polygon'].unique())\n",
    "\n",
    "common_polygons = set(df_indices['polygon'].unique()) & set(df_area['polygon'].unique())\n",
    "print(f\"\\nPolygons common to both df_indices and df_area: {list(common_polygons)}\")\n",
    "\n",
    "missing_in_area_file = set(df_indices['polygon'].unique()) - set(df_area['polygon'].unique())\n",
    "if missing_in_area_file:\n",
    "    print(f\"Polygons found in drone data (df_indices) but NOT in polygon_areas.csv: {list(missing_in_area_file)}\")\n",
    "else:\n",
    "    print(\"All polygons from drone data (df_indices) are present in polygon_areas.csv.\")\n",
    "print(\"--- Debugging Polygon Names for Merge End ---\\n\")\n",
    "\n",
    "# Merge area data with drone vegetation index features.\n",
    "df_indices = pd.merge(df_indices, df_area, on='polygon', how='left')\n",
    "\n",
    "# Check for missing area values after merge.\n",
    "missing_area = df_indices[df_indices['area_ha'].isna()]\n",
    "if not missing_area.empty:\n",
    "    print(\"Warning: Some polygons in your drone dataset are missing area values (area_ha is NaN) after merge:\")\n",
    "    print(missing_area['polygon'].unique())\n",
    "    print(\"This indicates a mismatch in polygon names or missing entries in 'polygon_areas.csv'.\")\n",
    "\n",
    "print(\"✅ Merged drone vegetation index features with polygon area data:\")\n",
    "print(df_indices.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad4678f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged vegetation index + area data:\n",
      "  polygon  date    filename   mean_ExG  mean_GLI  mean_VARI   mean_TGI  \\\n",
      "0  Munda1  None  Munda1.tif  29.988546  0.084960   0.005946  18.162916   \n",
      "1  Munda2  None  Munda2.tif  16.802973  0.040981  -0.038630  11.502631   \n",
      "2  Munda3  None  Munda3.tif  29.551044  0.077395  -0.044689  19.322803   \n",
      "3  Munda4  None  Munda4.tif  21.299973  0.058161  -0.026116  13.652133   \n",
      "4  Munda5  None  Munda5.tif  29.342831  0.077635   0.011176  17.655802   \n",
      "\n",
      "   mean_CIVE   mean_Red  mean_Green  mean_Blue   area_ha  \n",
      "0  18.727116  72.174706   72.766083  43.368896  0.101974  \n",
      "1  18.757044  64.890701   59.196167  36.698555  0.339510  \n",
      "2  18.732512  88.398834   82.504814  47.059834  0.308073  \n",
      "3  18.746469  58.684658   55.688457  31.392330  0.289367  \n",
      "4  18.732330  68.685951   69.791962  41.555145  0.185417  \n",
      "\n",
      "Unique polygons found in drone data:\n",
      "['Munda1' 'Munda2' 'Munda3' 'Munda4' 'Munda5' 'Munda6' 'Munda7']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "image_folder = r\"D:\\Yield\\LR\\DroneData\"\n",
    "records = []\n",
    "\n",
    "# List all .tif files within the specified drone image folder\n",
    "files = [f for f in os.listdir(image_folder) if f.endswith('.tif')]\n",
    "files.sort() # Sort files to ensure consistent order\n",
    "\n",
    "for file in files:\n",
    "    file_path = os.path.join(image_folder, file)\n",
    "\n",
    "    try:\n",
    "        with rasterio.open(file_path) as src:\n",
    "            # Assume Red=1, Green=2, Blue=3 for standard RGB drone images.\n",
    "            # Adjust these band indices (e.g., src.read(3) for Red, src.read(2) for Green, etc.)\n",
    "            # if your drone imagery has a different band order (e.g., BGR).\n",
    "            red = src.read(1).astype('float32')\n",
    "            green = src.read(2).astype('float32')\n",
    "            blue = src.read(3).astype('float32')\n",
    "\n",
    "        # === Calculate Vegetation Indices for RGB only ===\n",
    "        # Excess Green (ExG): Highlights green vegetation\n",
    "        exg = 2 * green - red - blue\n",
    "        # Green Leaf Index (GLI): Sensitive to green biomass\n",
    "        gli = (2 * green - red - blue) / (2 * green + red + blue + 1e-10)\n",
    "        # Visible Atmospherically Resistant Index (VARI): Reduces atmospheric effects\n",
    "        vari = (green - red) / (green + red - blue + 1e-10)\n",
    "\n",
    "        # --- NEW INDICES ---\n",
    "        # Triangular Greenness Index (TGI): Estimates chlorophyll content\n",
    "        tgi = green - 0.39 * red - 0.61 * blue\n",
    "\n",
    "        # Color Index of Vegetation (CIVE): Designed to be robust to light conditions\n",
    "        # Normalize RGB first for CIVE\n",
    "        sum_rgb = red + green + blue + 1e-10 # Add epsilon to avoid division by zero\n",
    "        r_norm = red / sum_rgb\n",
    "        g_norm = green / sum_rgb\n",
    "        b_norm = blue / sum_rgb\n",
    "        cive = 0.441 * r_norm - 0.881 * g_norm + 0.385 * b_norm + 18.78745\n",
    "        # --- END NEW INDICES ---\n",
    "\n",
    "        # Clean up potential infinite or NaN values that can arise from divisions\n",
    "        # np.isfinite() checks for numbers that are not NaN or infinity.\n",
    "        exg = np.where(np.isfinite(exg), exg, np.nan)\n",
    "        gli = np.where(np.isfinite(gli), gli, np.nan)\n",
    "        vari = np.where(np.isfinite(vari), vari, np.nan)\n",
    "        tgi = np.where(np.isfinite(tgi), tgi, np.nan) # Clean TGI\n",
    "        cive = np.where(np.isfinite(cive), cive, np.nan) # Clean CIVE\n",
    "\n",
    "        # Calculate the mean value for each vegetation index and raw band across the image\n",
    "        mean_exg = np.nanmean(exg)\n",
    "        mean_gli = np.nanmean(gli)\n",
    "        mean_vari = np.nanmean(vari)\n",
    "        mean_tgi = np.nanmean(tgi) \n",
    "        mean_cive = np.nanmean(cive)\n",
    "        mean_red = np.nanmean(red)\n",
    "        mean_green = np.nanmean(green)\n",
    "        mean_blue = np.nanmean(blue)\n",
    "\n",
    "        # === Extract metadata (polygon name) from filename ===\n",
    "        # Based on your input, filenames are simply \"PolygonName.tif\" (e.g., \"Munda1.tif\")\n",
    "        # The date is not present in the filename.\n",
    "        try:\n",
    "            # Extract polygon name by splitting on the first '.' (before the file extension)\n",
    "            polygon = file.split('.')[0]\n",
    "            date = None # Explicitly set date to None as it's not in the filename\n",
    "        except Exception as e:\n",
    "            # Catch any unexpected errors during metadata extraction\n",
    "            print(f\"Error extracting polygon name from '{file}': {e}. Polygon set to None.\")\n",
    "            polygon = None\n",
    "            date = None\n",
    "\n",
    "        # Append the extracted features and metadata to the records list\n",
    "        records.append({\n",
    "            'polygon': polygon,\n",
    "            'date': date, # Date will be None\n",
    "            'filename': file,\n",
    "            'mean_ExG': mean_exg,\n",
    "            'mean_GLI': mean_gli,\n",
    "            'mean_VARI': mean_vari,\n",
    "            'mean_TGI': mean_tgi, \n",
    "            'mean_CIVE': mean_cive, \n",
    "            'mean_Red': mean_red,\n",
    "            'mean_Green': mean_green,\n",
    "            'mean_Blue': mean_blue\n",
    "        })\n",
    "\n",
    "    except rasterio.errors.RasterioIOError as e:\n",
    "        print(f\"Error opening or reading raster file '{file_path}': {e}. Skipping this file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while processing '{file_path}': {e}. Skipping this file.\")\n",
    "\n",
    "# === Step 2: Create DataFrame from collected records ===\n",
    "df_indices = pd.DataFrame(records)\n",
    "\n",
    "# Load area data\n",
    "df_area = pd.read_csv(\"polygon_areas.csv\")\n",
    "\n",
    "# Merge area with vegetation index data\n",
    "df_indices = pd.merge(df_indices, df_area, on='polygon', how='left')\n",
    "\n",
    "# Check for any polygons without area info\n",
    "missing_area = df_indices[df_indices['area_ha'].isna()]\n",
    "if not missing_area.empty:\n",
    "    print(\"Warning: Some polygons are missing area values:\")\n",
    "    print(missing_area['polygon'].unique())\n",
    "\n",
    "# Show result\n",
    "print(\"Merged vegetation index + area data:\")\n",
    "print(df_indices.head())\n",
    "\n",
    "# Also print unique polygons found to confirm correct extraction\n",
    "print(\"\\nUnique polygons found in drone data:\")\n",
    "print(df_indices['polygon'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8b9c3572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted vegetation indices and variability features from drone imagery:\n",
      "  polygon  date    filename   mean_ExG  mean_GLI  mean_VARI   mean_TGI  \\\n",
      "0  Munda1  None  Munda1.tif  53.546490  0.150254   0.023449  32.256836   \n",
      "1  Munda2  None  Munda2.tif  41.190311  0.100614  -0.054592  27.334774   \n",
      "2  Munda3  None  Munda3.tif  46.276634  0.120017  -0.044731  29.833282   \n",
      "3  Munda4  None  Munda4.tif  49.481083  0.133959  -0.028991  31.134373   \n",
      "4  Munda5  None  Munda5.tif  58.587349  0.154823   0.029838  34.988140   \n",
      "\n",
      "   mean_CIVE    mean_Red  mean_Green  mean_Blue    std_ExG  std_Green  \\\n",
      "0  18.681017  126.814560  128.662354  76.963638  24.812695  60.642323   \n",
      "1  18.714027  146.354401  136.314972  85.085243  19.989790  57.192657   \n",
      "2  18.702696  133.024643  125.731285  72.161270  19.136429  53.785713   \n",
      "3  18.693714  128.715103  124.392838  70.589462  21.671139  60.535816   \n",
      "4  18.678141  129.899765  133.309601  78.132088  24.228174  53.942223   \n",
      "\n",
      "   percent_green_cover  \n",
      "0            55.827362  \n",
      "1            40.566476  \n",
      "2            63.499409  \n",
      "3            42.832816  \n",
      "4            49.963518  \n",
      "\n",
      "Unique polygons found in drone data:\n",
      "['Munda1' 'Munda2' 'Munda3' 'Munda4' 'Munda5' 'Munda6' 'Munda7']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# === Step 1: Process all images in Drone folder ===\n",
    "# Base folder containing all drone image files (e.g., Munda1.tif, Munda2.tif, etc.)\n",
    "image_folder = r\"D:\\Yield\\LR\\DroneData\"\n",
    "records = []\n",
    "\n",
    "# List all .tif files within the specified drone image folder\n",
    "files = [f for f in os.listdir(image_folder) if f.endswith('.tif')]\n",
    "files.sort() # Sort files to ensure consistent order\n",
    "\n",
    "for file in files:\n",
    "    file_path = os.path.join(image_folder, file)\n",
    "\n",
    "    try:\n",
    "        with rasterio.open(file_path) as src:\n",
    "            # Assume Red=1, Green=2, Blue=3 for standard RGB drone images.\n",
    "            # Adjust these band indices (e.g., src.read(3) for Red, src.read(2) for Green, etc.)\n",
    "            # if your drone imagery has a different band order (e.g., BGR).\n",
    "            red = src.read(1).astype('float32')\n",
    "            green = src.read(2).astype('float32')\n",
    "            blue = src.read(3).astype('float32')\n",
    "\n",
    "        # === Calculate Vegetation Indices for RGB only ===\n",
    "        exg = 2 * green - red - blue\n",
    "        gli = (2 * green - red - blue) / (2 * green + red + blue + 1e-10)\n",
    "        vari = (green - red) / (green + red - blue + 1e-10)\n",
    "        tgi = green - 0.39 * red - 0.61 * blue\n",
    "        sum_rgb = red + green + blue + 1e-10\n",
    "        r_norm = red / sum_rgb\n",
    "        g_norm = green / sum_rgb\n",
    "        b_norm = blue / sum_rgb\n",
    "        cive = 0.441 * r_norm - 0.881 * g_norm + 0.385 * b_norm + 18.78745\n",
    "\n",
    "        # Clean up potential infinite or NaN values from initial calculations\n",
    "        exg = np.where(np.isfinite(exg), exg, np.nan)\n",
    "        gli = np.where(np.isfinite(gli), gli, np.nan)\n",
    "        vari = np.where(np.isfinite(vari), vari, np.nan)\n",
    "        tgi = np.where(np.isfinite(tgi), tgi, np.nan)\n",
    "        cive = np.where(np.isfinite(cive), cive, np.nan)\n",
    "\n",
    "        # === Implement Masking for Non-Maize Areas ===\n",
    "        # Create a mask for green vegetation (e.g., ExG > a certain threshold)\n",
    "        # Adjust the threshold (e.g., 10, 20, 30) based on your image characteristics\n",
    "        # A simple threshold like ExG > 0 is a good starting point to exclude bare soil/non-vegetation.\n",
    "        vegetation_mask = exg > 10 # Example threshold, adjust as needed (e.g., 0, 10, 20)\n",
    "\n",
    "        # Calculate percentage of green cover\n",
    "        # Count pixels where vegetation_mask is True and divide by total valid pixels\n",
    "        total_valid_pixels = np.sum(~np.isnan(exg)) # Count non-NaN pixels for the denominator\n",
    "        if total_valid_pixels > 0:\n",
    "            percent_green_cover = (np.sum(vegetation_mask) / total_valid_pixels) * 100\n",
    "        else:\n",
    "            percent_green_cover = np.nan # No valid pixels to calculate cover\n",
    "\n",
    "        # Apply the mask to all bands and indices\n",
    "        red_masked = np.where(vegetation_mask, red, np.nan)\n",
    "        green_masked = np.where(vegetation_mask, green, np.nan)\n",
    "        blue_masked = np.where(vegetation_mask, blue, np.nan)\n",
    "        exg_masked = np.where(vegetation_mask, exg, np.nan)\n",
    "        gli_masked = np.where(vegetation_mask, gli, np.nan)\n",
    "        vari_masked = np.where(vegetation_mask, vari, np.nan)\n",
    "        tgi_masked = np.where(vegetation_mask, tgi, np.nan)\n",
    "        cive_masked = np.where(vegetation_mask, cive, np.nan)\n",
    "\n",
    "        # Calculate the mean value for each masked vegetation index and raw band\n",
    "        # Use np.nanmean to ignore NaN values introduced by masking\n",
    "        mean_exg_masked = np.nanmean(exg_masked)\n",
    "        mean_gli_masked = np.nanmean(gli_masked)\n",
    "        mean_vari_masked = np.nanmean(vari_masked)\n",
    "        mean_tgi_masked = np.nanmean(tgi_masked)\n",
    "        mean_cive_masked = np.nanmean(cive_masked)\n",
    "        mean_red_masked = np.nanmean(red_masked)\n",
    "        mean_green_masked = np.nanmean(green_masked)\n",
    "        mean_blue_masked = np.nanmean(blue_masked)\n",
    "\n",
    "        # === Add Simple Variability Features (Standard Deviation) ===\n",
    "        # Calculate standard deviation for masked ExG and Green band\n",
    "        std_exg_masked = np.nanstd(exg_masked)\n",
    "        std_green_masked = np.nanstd(green_masked)\n",
    "\n",
    "        # === Extract metadata (polygon name) from filename ===\n",
    "        try:\n",
    "            polygon = file.split('.')[0]\n",
    "            date = None # Date not present in filename\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting polygon name from '{file}': {e}. Polygon set to None.\")\n",
    "            polygon = None\n",
    "            date = None\n",
    "\n",
    "        # Append the extracted features and metadata to the records list\n",
    "        records.append({\n",
    "            'polygon': polygon,\n",
    "            'date': date,\n",
    "            'filename': file,\n",
    "            'mean_ExG': mean_exg_masked, # Now using masked mean\n",
    "            'mean_GLI': mean_gli_masked, # Now using masked mean\n",
    "            'mean_VARI': mean_vari_masked, # Now using masked mean\n",
    "            'mean_TGI': mean_tgi_masked, # Now using masked mean\n",
    "            'mean_CIVE': mean_cive_masked, # Now using masked mean\n",
    "            'mean_Red': mean_red_masked, # Now using masked mean\n",
    "            'mean_Green': mean_green_masked, # Now using masked mean\n",
    "            'mean_Blue': mean_blue_masked, # Now using masked mean\n",
    "            'std_ExG': std_exg_masked, # New variability feature\n",
    "            'std_Green': std_green_masked, # New variability feature\n",
    "            'percent_green_cover': percent_green_cover # New green cover feature\n",
    "        })\n",
    "\n",
    "    except rasterio.errors.RasterioIOError as e:\n",
    "        print(f\"Error opening or reading raster file '{file_path}': {e}. Skipping this file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while processing '{file_path}': {e}. Skipping this file.\")\n",
    "\n",
    "# === Step 2: Create DataFrame from collected records ===\n",
    "df_indices = pd.DataFrame(records)\n",
    "\n",
    "# Show extracted features for verification\n",
    "print(\"✅ Extracted vegetation indices and variability features from drone imagery:\")\n",
    "print(df_indices.head())\n",
    "\n",
    "# Also print unique polygons found to confirm correct extraction\n",
    "print(\"\\nUnique polygons found in drone data:\")\n",
    "print(df_indices['polygon'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f441037c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns after merging with yield data:\n",
      "['polygon', 'date', 'filename', 'mean_ExG', 'mean_GLI', 'mean_VARI', 'mean_TGI', 'mean_CIVE', 'mean_Red', 'mean_Green', 'mean_Blue', 'std_ExG', 'std_Green', 'percent_green_cover', 'area_ha_x', 'area_ha_y', 'area_ha', 'yield_tons']\n",
      "\n",
      "✅ Per-image data ready for modeling (drone data - base for LOOCV):\n",
      "  polygon  date    filename   mean_ExG  mean_GLI  mean_VARI   mean_TGI  \\\n",
      "0  Munda1  None  Munda1.tif  53.546490  0.150254   0.023449  32.256836   \n",
      "1  Munda2  None  Munda2.tif  41.190311  0.100614  -0.054592  27.334774   \n",
      "2  Munda3  None  Munda3.tif  46.276634  0.120017  -0.044731  29.833282   \n",
      "3  Munda4  None  Munda4.tif  49.481083  0.133959  -0.028991  31.134373   \n",
      "4  Munda5  None  Munda5.tif  58.587349  0.154823   0.029838  34.988140   \n",
      "\n",
      "   mean_CIVE    mean_Red  mean_Green  mean_Blue    std_ExG  std_Green  \\\n",
      "0  18.681017  126.814560  128.662354  76.963638  24.812695  60.642323   \n",
      "1  18.714027  146.354401  136.314972  85.085243  19.989790  57.192657   \n",
      "2  18.702696  133.024643  125.731285  72.161270  19.136429  53.785713   \n",
      "3  18.693714  128.715103  124.392838  70.589462  21.671139  60.535816   \n",
      "4  18.678141  129.899765  133.309601  78.132088  24.228174  53.942223   \n",
      "\n",
      "   percent_green_cover  area_ha_x  area_ha_y   area_ha  yield_tons  \n",
      "0            55.827362   0.101974   0.101974  0.101974         0.9  \n",
      "1            40.566476   0.339510   0.339510  0.339510         1.3  \n",
      "2            63.499409   0.308073   0.308073  0.308073         1.4  \n",
      "3            42.832816   0.289367   0.289367  0.289367         0.7  \n",
      "4            49.963518   0.185417   0.185417  0.185417         1.1  \n",
      "\n",
      "Polygons included in the base dataset for LOOCV:\n",
      "['Munda1' 'Munda2' 'Munda3' 'Munda4' 'Munda5' 'Munda7']\n"
     ]
    }
   ],
   "source": [
    "# Load yield data.\n",
    "df_yield = pd.read_csv(\"yield_tons.csv\")\n",
    "\n",
    "# Merge drone feature data with yield data.\n",
    "df = pd.merge(df_indices, df_yield, on='polygon', how='inner')\n",
    "\n",
    "# Check available columns after merging.\n",
    "print(\"Available columns after merging with yield data:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Define all features, including the new ones, for dropping rows with missing data.\n",
    "all_features_for_dropna = [\n",
    "    'mean_ExG', 'mean_GLI', 'mean_VARI',\n",
    "    'mean_TGI', 'mean_CIVE',\n",
    "    'mean_Red', 'mean_Green', 'mean_Blue',\n",
    "    'std_ExG', 'std_Green', 'percent_green_cover', # Include new features\n",
    "    'area_ha', 'yield_tons'\n",
    "]\n",
    "\n",
    "# Drop rows with any missing data in the specified columns.\n",
    "df = df.dropna(subset=all_features_for_dropna)\n",
    "\n",
    "print(\"\\n✅ Per-image data ready for modeling (drone data - base for LOOCV):\")\n",
    "print(df.head())\n",
    "print(\"\\nPolygons included in the base dataset for LOOCV:\")\n",
    "print(df['polygon'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e8760562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Debugging df before df_summary creation ---\n",
      "Columns in df before creating df_summary: ['polygon', 'date', 'filename', 'mean_ExG', 'mean_GLI', 'mean_VARI', 'mean_TGI', 'mean_CIVE', 'mean_Red', 'mean_Green', 'mean_Blue', 'std_ExG', 'std_Green', 'percent_green_cover', 'area_ha_x', 'area_ha_y', 'area_ha', 'yield_tons']\n",
      "--- Debugging df END ---\n",
      "\n",
      "\n",
      "✅ Image-level training dataset (for drone data - LOOCV base):\n",
      "  polygon   mean_ExG  mean_GLI  mean_VARI   mean_TGI  mean_CIVE    mean_Red  \\\n",
      "0  Munda1  53.546490  0.150254   0.023449  32.256836  18.681017  126.814560   \n",
      "1  Munda2  41.190311  0.100614  -0.054592  27.334774  18.714027  146.354401   \n",
      "2  Munda3  46.276634  0.120017  -0.044731  29.833282  18.702696  133.024643   \n",
      "3  Munda4  49.481083  0.133959  -0.028991  31.134373  18.693714  128.715103   \n",
      "4  Munda5  58.587349  0.154823   0.029838  34.988140  18.678141  129.899765   \n",
      "\n",
      "   mean_Green  mean_Blue    std_ExG  std_Green  percent_green_cover   area_ha  \\\n",
      "0  128.662354  76.963638  24.812695  60.642323            55.827362  0.101974   \n",
      "1  136.314972  85.085243  19.989790  57.192657            40.566476  0.339510   \n",
      "2  125.731285  72.161270  19.136429  53.785713            63.499409  0.308073   \n",
      "3  124.392838  70.589462  21.671139  60.535816            42.832816  0.289367   \n",
      "4  133.309601  78.132088  24.228174  53.942223            49.963518  0.185417   \n",
      "\n",
      "   yield_tons    filename  \n",
      "0         0.9  Munda1.tif  \n",
      "1         1.3  Munda2.tif  \n",
      "2         1.4  Munda3.tif  \n",
      "3         0.7  Munda4.tif  \n",
      "4         1.1  Munda5.tif  \n",
      "\n",
      "Polygons in df_summary (all available for LOOCV):\n",
      "['Munda1' 'Munda2' 'Munda3' 'Munda4' 'Munda5' 'Munda7']\n"
     ]
    }
   ],
   "source": [
    "# --- DEBUGGING ADDITION START ---\n",
    "print(\"\\n--- Debugging df before df_summary creation ---\")\n",
    "print(f\"Columns in df before creating df_summary: {df.columns.tolist()}\")\n",
    "print(\"--- Debugging df END ---\\n\")\n",
    "# --- DEBUGGING ADDITION END ---\n",
    "\n",
    "# Select only the columns needed for training and testing in the LOOCV loop.\n",
    "# This ensures df_summary contains all the new features.\n",
    "df_summary = df[[\n",
    "    'polygon',\n",
    "    'mean_ExG',\n",
    "    'mean_GLI',\n",
    "    'mean_VARI',\n",
    "    'mean_TGI',\n",
    "    'mean_CIVE',\n",
    "    'mean_Red',\n",
    "    'mean_Green',\n",
    "    'mean_Blue',\n",
    "    'std_ExG',           # Include new feature\n",
    "    'std_Green',         # Include new feature\n",
    "    'percent_green_cover',# Include new feature\n",
    "    'area_ha',\n",
    "    'yield_tons',\n",
    "    'filename'\n",
    "]].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "# Drop any rows with missing values as a final clean-up step for the summary DataFrame.\n",
    "df_summary = df_summary.dropna()\n",
    "\n",
    "print(\"\\n✅ Image-level training dataset (for drone data - LOOCV base):\")\n",
    "print(df_summary.head())\n",
    "print(\"\\nPolygons in df_summary (all available for LOOCV):\")\n",
    "print(df_summary['polygon'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f933b476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Debugging df_summary before Custom Cross-Validation ---\n",
      "Shape of df_summary: (6, 15)\n",
      "Columns in df_summary: ['polygon', 'mean_ExG', 'mean_GLI', 'mean_VARI', 'mean_TGI', 'mean_CIVE', 'mean_Red', 'mean_Green', 'mean_Blue', 'std_ExG', 'std_Green', 'percent_green_cover', 'area_ha', 'yield_tons', 'filename']\n",
      "Unique polygons in df_summary: ['Munda1', 'Munda2', 'Munda3', 'Munda4', 'Munda5', 'Munda7']\n",
      "--- Debugging df_summary END ---\n",
      "\n",
      "Starting Custom Cross-Validation: Training on 5 fields, Testing on 1 field.\n",
      "Total unique polygons available: 6\n",
      "Polygons in dataset: ['Munda1', 'Munda2', 'Munda3', 'Munda4', 'Munda5', 'Munda7']\n",
      "\n",
      "--- Fold 1: Testing on 'Munda1' ---\n",
      "  Training polygons for this fold: ['Munda2', 'Munda3', 'Munda4', 'Munda5', 'Munda7']\n",
      "  Unused polygons in this fold: []\n",
      "  Warning: y_test for 'Munda1' has no variance. Test R² will be NaN for this fold.\n",
      "  Training R²: 1.0000 | Training RMSE: 0.0000 | Training MAE: 0.0000\n",
      "  Test R²: nan | Test RMSE: 1.3095 | Test MAE: 1.3095\n",
      "\n",
      "--- Fold 2: Testing on 'Munda2' ---\n",
      "  Training polygons for this fold: ['Munda1', 'Munda3', 'Munda4', 'Munda5', 'Munda7']\n",
      "  Unused polygons in this fold: []\n",
      "  Warning: y_test for 'Munda2' has no variance. Test R² will be NaN for this fold.\n",
      "  Training R²: 1.0000 | Training RMSE: 0.0000 | Training MAE: 0.0000\n",
      "  Test R²: nan | Test RMSE: 2.7668 | Test MAE: 2.7668\n",
      "\n",
      "--- Fold 3: Testing on 'Munda3' ---\n",
      "  Training polygons for this fold: ['Munda1', 'Munda2', 'Munda4', 'Munda5', 'Munda7']\n",
      "  Unused polygons in this fold: []\n",
      "  Warning: y_test for 'Munda3' has no variance. Test R² will be NaN for this fold.\n",
      "  Training R²: 1.0000 | Training RMSE: 0.0000 | Training MAE: 0.0000\n",
      "  Test R²: nan | Test RMSE: 26.4989 | Test MAE: 26.4989\n",
      "\n",
      "--- Fold 4: Testing on 'Munda4' ---\n",
      "  Training polygons for this fold: ['Munda1', 'Munda2', 'Munda3', 'Munda5', 'Munda7']\n",
      "  Unused polygons in this fold: []\n",
      "  Warning: y_test for 'Munda4' has no variance. Test R² will be NaN for this fold.\n",
      "  Training R²: 1.0000 | Training RMSE: 0.0000 | Training MAE: 0.0000\n",
      "  Test R²: nan | Test RMSE: 3.1672 | Test MAE: 3.1672\n",
      "\n",
      "--- Fold 5: Testing on 'Munda5' ---\n",
      "  Training polygons for this fold: ['Munda1', 'Munda2', 'Munda3', 'Munda4', 'Munda7']\n",
      "  Unused polygons in this fold: []\n",
      "  Warning: y_test for 'Munda5' has no variance. Test R² will be NaN for this fold.\n",
      "  Training R²: 1.0000 | Training RMSE: 0.0000 | Training MAE: 0.0000\n",
      "  Test R²: nan | Test RMSE: 10.0866 | Test MAE: 10.0866\n",
      "\n",
      "--- Fold 6: Testing on 'Munda7' ---\n",
      "  Training polygons for this fold: ['Munda1', 'Munda2', 'Munda3', 'Munda4', 'Munda5']\n",
      "  Unused polygons in this fold: []\n",
      "  Warning: y_test for 'Munda7' has no variance. Test R² will be NaN for this fold.\n",
      "  Training R²: 1.0000 | Training RMSE: 0.0000 | Training MAE: 0.0000\n",
      "  Test R²: nan | Test RMSE: 1.5973 | Test MAE: 1.5973\n",
      "\n",
      "--- Average Model Performance (Custom Cross-Validation Results) ---\n",
      "Total number of folds completed: 6\n",
      "--- Test Set Averages ---\n",
      "Average Test R² Score: nan\n",
      "Average Test RMSE (tons/ha): 7.5711\n",
      "Average Test MAE (tons/ha): 7.5711\n",
      "\n",
      "--- Training Set Averages ---\n",
      "Average Training R² Score: 1.0000\n",
      "Average Training RMSE (tons/ha): 0.0000\n",
      "Average Training MAE (tons/ha): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_4652\\676301949.py:137: RuntimeWarning: Mean of empty slice\n",
      "  avg_test_r2 = np.nanmean(test_r2_scores)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, root_mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "\n",
    "# Define features for training and prediction.\n",
    "# These features now include masked means, standard deviations, and percentage green cover.\n",
    "features = [\n",
    "    'mean_ExG',\n",
    "    'mean_GLI',\n",
    "    'mean_VARI',\n",
    "    'mean_TGI',\n",
    "    'mean_CIVE',\n",
    "    'mean_Red',\n",
    "    'mean_Green',\n",
    "    'mean_Blue',\n",
    "    'std_ExG',           # Standard deviation of ExG\n",
    "    'std_Green',         # Standard deviation of Green band\n",
    "    'percent_green_cover'# Percentage of green cover\n",
    "]\n",
    "\n",
    "# Debugging information to check df_summary before cross-validation.\n",
    "print(\"\\n--- Debugging df_summary before Custom Cross-Validation ---\")\n",
    "print(f\"Shape of df_summary: {df_summary.shape}\")\n",
    "print(f\"Columns in df_summary: {df_summary.columns.tolist()}\")\n",
    "print(f\"Unique polygons in df_summary: {df_summary['polygon'].unique().tolist()}\")\n",
    "print(\"--- Debugging df_summary END ---\\n\")\n",
    "\n",
    "# Get all unique polygon identifiers from the df_summary DataFrame.\n",
    "unique_polygons = df_summary['polygon'].unique().tolist()\n",
    "num_total_polygons = len(unique_polygons)\n",
    "\n",
    "# Define the desired sizes for the training and test sets in each fold.\n",
    "train_set_size = 5\n",
    "test_set_size = 1\n",
    "\n",
    "# Initialize lists to store performance metrics for both test and training sets.\n",
    "test_r2_scores = []\n",
    "test_rmse_scores = []\n",
    "test_mae_scores = []\n",
    "\n",
    "train_r2_scores = []\n",
    "train_rmse_scores = []\n",
    "train_mae_scores = []\n",
    "\n",
    "print(f\"Starting Custom Cross-Validation: Training on {train_set_size} fields, Testing on {test_set_size} field.\")\n",
    "print(f\"Total unique polygons available: {num_total_polygons}\")\n",
    "print(f\"Polygons in dataset: {unique_polygons}\")\n",
    "\n",
    "fold_counter = 0\n",
    "\n",
    "# Outer loop: Iterate through each unique polygon, setting it as the test polygon for a set of folds.\n",
    "for test_polygon in unique_polygons:\n",
    "    # Create the test set DataFrame for the current test polygon.\n",
    "    df_test_fold = df_summary[df_summary['polygon'] == test_polygon].copy()\n",
    "\n",
    "    # Skip if the test set is empty.\n",
    "    if df_test_fold.empty:\n",
    "        print(f\"Warning: Test set is empty for '{test_polygon}'. Skipping folds with this test polygon.\")\n",
    "        continue\n",
    "\n",
    "    # Identify candidate polygons for the training set (all except the current test polygon).\n",
    "    candidate_train_polygons = [p for p in unique_polygons if p != test_polygon]\n",
    "\n",
    "    # Inner loop: Generate all combinations of 'train_set_size' polygons from candidates for training.\n",
    "    for train_subset_polygons in combinations(candidate_train_polygons, train_set_size):\n",
    "        fold_counter += 1\n",
    "        print(f\"\\n--- Fold {fold_counter}: Testing on '{test_polygon}' ---\")\n",
    "        print(f\"  Training polygons for this fold: {list(train_subset_polygons)}\")\n",
    "\n",
    "        # Identify and print polygons not used in this specific fold.\n",
    "        unused_polygons = [p for p in candidate_train_polygons if p not in train_subset_polygons]\n",
    "        print(f\"  Unused polygons in this fold: {unused_polygons}\")\n",
    "\n",
    "        # Create the training set DataFrame for the current fold.\n",
    "        df_train_fold = df_summary[df_summary['polygon'].isin(train_subset_polygons)].copy()\n",
    "\n",
    "        # Skip if the training set is empty.\n",
    "        if df_train_fold.empty:\n",
    "            print(f\"Warning: Training set is empty for fold {fold_counter}. Skipping this fold.\")\n",
    "            continue\n",
    "\n",
    "        # Define X (features) and y (target) for the training set.\n",
    "        X_train = df_train_fold[features]\n",
    "        y_train = df_train_fold['yield_tons']\n",
    "\n",
    "        # Define X (features) and y (target) for the test set.\n",
    "        X_test = df_test_fold[features]\n",
    "        y_test = df_test_fold['yield_tons']\n",
    "\n",
    "        # Initialize and train the linear regression model.\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Calculate Training Set Performance.\n",
    "        y_pred_train = model.predict(X_train)\n",
    "\n",
    "        # Calculate R² for training, handling cases with no variance in y_train.\n",
    "        if len(y_train.unique()) > 1:\n",
    "            fold_train_r2 = r2_score(y_train, y_pred_train)\n",
    "        else:\n",
    "            fold_train_r2 = np.nan\n",
    "            print(f\"  Warning: y_train for fold {fold_counter} has no variance. Training R² will be NaN.\")\n",
    "\n",
    "        fold_train_rmse = root_mean_squared_error(y_train, y_pred_train)\n",
    "        fold_train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "\n",
    "        train_r2_scores.append(fold_train_r2)\n",
    "        train_rmse_scores.append(fold_train_rmse)\n",
    "        train_mae_scores.append(fold_train_mae)\n",
    "\n",
    "        # Calculate Test Set Performance.\n",
    "        y_pred_test = model.predict(X_test)\n",
    "\n",
    "        # Calculate R² for testing, handling cases with no variance in y_test.\n",
    "        if len(y_test.unique()) > 1:\n",
    "            fold_test_r2 = r2_score(y_test, y_pred_test)\n",
    "        else:\n",
    "            fold_test_r2 = np.nan\n",
    "            print(f\"  Warning: y_test for '{test_polygon}' has no variance. Test R² will be NaN for this fold.\")\n",
    "\n",
    "        fold_test_rmse = root_mean_squared_error(y_test, y_pred_test)\n",
    "        fold_test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "\n",
    "        test_r2_scores.append(fold_test_r2)\n",
    "        test_rmse_scores.append(fold_test_rmse)\n",
    "        test_mae_scores.append(fold_test_mae)\n",
    "\n",
    "        # Print performance metrics for the current fold.\n",
    "        print(f\"  Training R²: {fold_train_r2:.4f} | Training RMSE: {fold_train_rmse:.4f} | Training MAE: {fold_train_mae:.4f}\")\n",
    "        print(f\"  Test R²: {fold_test_r2:.4f} | Test RMSE: {fold_test_rmse:.4f} | Test MAE: {fold_test_mae:.4f}\")\n",
    "\n",
    "\n",
    "# Calculate overall average performance metrics across all folds.\n",
    "# np.nanmean is used to correctly handle any NaN values.\n",
    "if test_r2_scores:\n",
    "    avg_test_r2 = np.nanmean(test_r2_scores)\n",
    "    avg_test_rmse = np.nanmean(test_rmse_scores)\n",
    "    avg_test_mae = np.nanmean(test_mae_scores)\n",
    "\n",
    "    avg_train_r2 = np.nanmean(train_r2_scores)\n",
    "    avg_train_rmse = np.nanmean(train_rmse_scores)\n",
    "    avg_train_mae = np.nanmean(train_mae_scores)\n",
    "\n",
    "    print(\"\\n--- Average Model Performance (Custom Cross-Validation Results) ---\")\n",
    "    print(f\"Total number of folds completed: {fold_counter}\")\n",
    "    print(\"--- Test Set Averages ---\")\n",
    "    print(f\"Average Test R² Score: {avg_test_r2:.4f}\")\n",
    "    print(f\"Average Test RMSE (tons/ha): {avg_test_rmse:.4f}\")\n",
    "    print(f\"Average Test MAE (tons/ha): {avg_test_mae:.4f}\")\n",
    "\n",
    "    print(\"\\n--- Training Set Averages ---\")\n",
    "    print(f\"Average Training R² Score: {avg_train_r2:.4f}\")\n",
    "    print(f\"Average Training RMSE (tons/ha): {avg_train_rmse:.4f}\")\n",
    "    print(f\"Average Training MAE (tons/ha): {avg_train_mae:.4f}\")\n",
    "else:\n",
    "    print(\"\\nNo valid cross-validation folds were completed. Check your data and parameters.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "748e1384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting yield prediction for all available polygons...\n",
      "\n",
      "--- Predicting for 'Munda1' ---\n",
      "Error predicting for 'Munda1': The feature names should match those that were passed during fit.\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- percent_green_cover\n",
      "- std_ExG\n",
      "- std_Green\n",
      ". Skipping prediction for this polygon.\n",
      "\n",
      "--- Predicting for 'Munda2' ---\n",
      "Error predicting for 'Munda2': The feature names should match those that were passed during fit.\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- percent_green_cover\n",
      "- std_ExG\n",
      "- std_Green\n",
      ". Skipping prediction for this polygon.\n",
      "\n",
      "--- Predicting for 'Munda3' ---\n",
      "Error predicting for 'Munda3': The feature names should match those that were passed during fit.\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- percent_green_cover\n",
      "- std_ExG\n",
      "- std_Green\n",
      ". Skipping prediction for this polygon.\n",
      "\n",
      "--- Predicting for 'Munda4' ---\n",
      "Error predicting for 'Munda4': The feature names should match those that were passed during fit.\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- percent_green_cover\n",
      "- std_ExG\n",
      "- std_Green\n",
      ". Skipping prediction for this polygon.\n",
      "\n",
      "--- Predicting for 'Munda5' ---\n",
      "Error predicting for 'Munda5': The feature names should match those that were passed during fit.\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- percent_green_cover\n",
      "- std_ExG\n",
      "- std_Green\n",
      ". Skipping prediction for this polygon.\n",
      "\n",
      "--- Predicting for 'Munda6' ---\n",
      "Error predicting for 'Munda6': The feature names should match those that were passed during fit.\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- percent_green_cover\n",
      "- std_ExG\n",
      "- std_Green\n",
      ". Skipping prediction for this polygon.\n",
      "\n",
      "--- Predicting for 'Munda7' ---\n",
      "Error predicting for 'Munda7': The feature names should match those that were passed during fit.\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- percent_green_cover\n",
      "- std_ExG\n",
      "- std_Green\n",
      ". Skipping prediction for this polygon.\n",
      "\n",
      "✅ Prediction process completed for all available polygons.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define features that your 'model' was trained on.\n",
    "# This list must match the 'features' list used in your model training (e.g., in the LOOCV cell).\n",
    "features = [\n",
    "    'mean_ExG',\n",
    "    'mean_GLI',\n",
    "    'mean_VARI',\n",
    "    'mean_TGI',\n",
    "    'mean_CIVE',\n",
    "    'mean_Red',\n",
    "    'mean_Green',\n",
    "    'mean_Blue'\n",
    "]\n",
    "\n",
    "# Get a list of all unique polygons present in df_indices.\n",
    "# df_indices should already contain all the drone features and the 'area_ha' column\n",
    "# merged from 'polygon_areas.csv'.\n",
    "unique_polygons_for_prediction = df_indices['polygon'].unique()\n",
    "\n",
    "print(\"Starting yield prediction for all available polygons...\")\n",
    "\n",
    "# Loop through each unique polygon to predict its yield.\n",
    "for polygon_name in unique_polygons_for_prediction:\n",
    "    print(f\"\\n--- Predicting for '{polygon_name}' ---\")\n",
    "\n",
    "    # Filter df_indices to get data only for the current polygon.\n",
    "    df_current_polygon = df_indices[df_indices['polygon'] == polygon_name].copy()\n",
    "\n",
    "    # Ensure the current polygon has data and all necessary features.\n",
    "    if df_current_polygon.empty:\n",
    "        print(f\"Warning: No data found for polygon '{polygon_name}'. Skipping prediction.\")\n",
    "        continue\n",
    "\n",
    "    # Check if all required features are present in the current polygon's data.\n",
    "    missing_features = [f for f in features if f not in df_current_polygon.columns]\n",
    "    if missing_features:\n",
    "        print(f\"Error: Missing features for polygon '{polygon_name}': {missing_features}. Skipping prediction.\")\n",
    "        continue\n",
    "\n",
    "    # Predict yield per hectare for each image within the current polygon.\n",
    "    # The 'model' object must have been trained in a previous cell.\n",
    "    # Ensure that the 'model' object is available in your environment.\n",
    "    try:\n",
    "        df_current_polygon['predicted_yield_per_ha'] = model.predict(df_current_polygon[features])\n",
    "    except NameError:\n",
    "        print(\"Error: 'model' is not defined. Please ensure the model training cell was run successfully.\")\n",
    "        break # Exit the loop if model is not trained\n",
    "    except Exception as e:\n",
    "        print(f\"Error predicting for '{polygon_name}': {e}. Skipping prediction for this polygon.\")\n",
    "        continue\n",
    "\n",
    "    # Calculate the average predicted yield per hectare across all images for this polygon.\n",
    "    avg_predicted_yield_per_ha = np.nanmean(df_current_polygon['predicted_yield_per_ha'])\n",
    "\n",
    "    # Get the polygon area for the current polygon.\n",
    "    # This assumes 'area_ha' column exists and is not NaN for this polygon.\n",
    "    # We take the mean in case there are multiple entries for the same polygon, though ideally 'area_ha' should be constant.\n",
    "    if 'area_ha' in df_current_polygon.columns and not df_current_polygon['area_ha'].empty and not pd.isna(df_current_polygon['area_ha'].iloc[0]):\n",
    "        polygon_area = df_current_polygon['area_ha'].iloc[0] # Assuming area is constant per polygon\n",
    "\n",
    "        # Calculate the total predicted yield in tons for the entire polygon.\n",
    "        total_predicted_yield = avg_predicted_yield_per_ha * polygon_area\n",
    "\n",
    "        # Print the results for the current polygon.\n",
    "        print(f\"  Average predicted yield per hectare: {avg_predicted_yield_per_ha:.3f} tons/ha\")\n",
    "        print(f\"  Total predicted maize yield: {total_predicted_yield:.3f} tons\")\n",
    "    else:\n",
    "        # Handle cases where 'area_ha' is missing or invalid for the current polygon.\n",
    "        print(f\"  ⚠️ Warning: '{polygon_name}' 'area_ha' is missing or invalid. Cannot calculate total predicted yield.\")\n",
    "        print(f\"  Average predicted yield per hectare: {avg_predicted_yield_per_ha:.3f} tons/ha (Total yield calculation requires area)\")\n",
    "\n",
    "print(\"\\n✅ Prediction process completed for all available polygons.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a609c2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting yield prediction for 'Munda6' only...\n",
      "  Average predicted yield per hectare: 1.485 tons/ha\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # Ensure pandas is imported if not already in the environment\n",
    "import numpy as np # Ensure numpy is imported for np.nanmean\n",
    "\n",
    "# Define features that your 'model' was trained on.\n",
    "# This list must match the 'features' list used in your model training (e.g., in the LOOCV cell).\n",
    "features = [\n",
    "    'mean_ExG',\n",
    "    'mean_GLI',\n",
    "    'mean_VARI',\n",
    "    'mean_TGI',\n",
    "    'mean_CIVE',\n",
    "    'mean_Red',\n",
    "    'mean_Green',\n",
    "    'mean_Blue',\n",
    "    'std_ExG',            # Standard deviation of ExG\n",
    "    'std_Green',          # Standard deviation of Green band\n",
    "    'percent_green_cover' # Percentage of green cover\n",
    "]\n",
    "\n",
    "# Define the specific polygon for which we want to predict the yield.\n",
    "polygon_to_predict = 'Munda6'\n",
    "\n",
    "print(f\"Starting yield prediction for '{polygon_to_predict}' only...\")\n",
    "\n",
    "# Filter df_indices to get data only for the specified polygon.\n",
    "# df_indices should already contain all the drone features and the 'area_ha' column\n",
    "# merged from 'polygon_areas.csv'.\n",
    "df_specific_polygon = df_indices[df_indices['polygon'] == polygon_to_predict].copy()\n",
    "\n",
    "# Ensure the specific polygon has data and all necessary features.\n",
    "if df_specific_polygon.empty:\n",
    "    print(f\"Error: No data found for polygon '{polygon_to_predict}'. Cannot proceed with prediction.\")\n",
    "else:\n",
    "    # Check if all required features are present in the polygon's data.\n",
    "    missing_features = [f for f in features if f not in df_specific_polygon.columns]\n",
    "    if missing_features:\n",
    "        print(f\"Error: Missing features for polygon '{polygon_to_predict}': {missing_features}. Cannot proceed with prediction.\")\n",
    "    else:\n",
    "        # Predict yield per hectare for each image within this polygon.\n",
    "        # The 'model' object must have been trained in a previous cell (e.g., the LOOCV cell).\n",
    "        # Ensure that the 'model' object is available in your environment.\n",
    "        try:\n",
    "            df_specific_polygon['predicted_yield_per_ha'] = model.predict(df_specific_polygon[features])\n",
    "        except NameError:\n",
    "            print(\"Error: 'model' is not defined. Please ensure the model training cell was run successfully before running this prediction.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred during prediction for '{polygon_to_predict}': {e}.\")\n",
    "        else:\n",
    "            # Calculate the average predicted yield per hectare across all images for this polygon.\n",
    "            avg_predicted_yield_per_ha = np.nanmean(df_specific_polygon['predicted_yield_per_ha'])\n",
    "\n",
    "            # Get the polygon area for the current polygon.\n",
    "            # This assumes 'area_ha' column exists and is not NaN for this polygon.\n",
    "            # We take the first value as area should be constant for a given polygon.\n",
    "            if 'area_ha' in df_specific_polygon.columns and not df_specific_polygon['area_ha'].empty and not pd.isna(df_specific_polygon['area_ha'].iloc[0]):\n",
    "                polygon_area = df_specific_polygon['area_ha'].iloc[0]\n",
    "\n",
    "                # Calculate the total predicted yield in tons for the entire polygon.\n",
    "                total_predicted_yield = avg_predicted_yield_per_ha * polygon_area\n",
    "\n",
    "                # Print the results for the specific polygon.\n",
    "                print(f\"  Average predicted yield per hectare: {avg_predicted_yield_per_ha:.3f} tons/ha\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
